{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "'./deeplearning.mplstyle' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pandiayyapamuruga/lib/python3.13/site-packages/matplotlib/style/core.py:129\u001b[39m, in \u001b[36muse\u001b[39m\u001b[34m(style)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     style = \u001b[43m_rc_params_in_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pandiayyapamuruga/lib/python3.13/site-packages/matplotlib/__init__.py:903\u001b[39m, in \u001b[36m_rc_params_in_file\u001b[39m\u001b[34m(fname, transform, fail_on_error)\u001b[39m\n\u001b[32m    902\u001b[39m rc_temp = {}\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_or_url(fname) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[32m    904\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pandiayyapamuruga/lib/python3.13/contextlib.py:141\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pandiayyapamuruga/lib/python3.13/site-packages/matplotlib/__init__.py:880\u001b[39m, in \u001b[36m_open_file_or_url\u001b[39m\u001b[34m(fname)\u001b[39m\n\u001b[32m    879\u001b[39m fname = os.path.expanduser(fname)\n\u001b[32m--> \u001b[39m\u001b[32m880\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    881\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './deeplearning.mplstyle'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m.\u001b[49m\u001b[43muse\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./deeplearning.mplstyle\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m np.set_printoptions(precision=\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# reduced display precision on numpy arrays\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pandiayyapamuruga/lib/python3.13/site-packages/matplotlib/style/core.py:131\u001b[39m, in \u001b[36muse\u001b[39m\u001b[34m(style)\u001b[39m\n\u001b[32m    129\u001b[39m         style = _rc_params_in_file(style)\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    132\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m is not a valid package style, path of style \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfile, URL of style file, or library style name (library \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstyles are listed in `style.available`)\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    135\u001b[39m filtered = {}\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: './deeplearning.mplstyle' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array([[2104,5,1,45],[1416,3,2,40],[852,2,1,35]])\n",
    "y_train=np.array([460,232,178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4) <class 'numpy.ndarray'>\n",
      "(3,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,type(x_train))\n",
    "print(y_train.shape,type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(459.9999976194083)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_single_loop(x,w,b):\n",
    "    f=0\n",
    "    n=x.shape[0]\n",
    "    for i in range(n):\n",
    "        f_i=w[i]*x[i]\n",
    "        f=f+f_i\n",
    "    f=f+b\n",
    "    return f\n",
    "        \n",
    "\n",
    "predict_single_loop(x_vec,w_init,b_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = x_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = x_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec,w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.5578904045996674e-12)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost_function(x,y,w,b):\n",
    "    j=0\n",
    "    m=x.shape[0]\n",
    "    for i in range(m):\n",
    "        f_i=np.dot(w,x[i])+b\n",
    "        j=j+((f_i-y[i])**2)\n",
    "    j=j/(2*m)\n",
    "    return j\n",
    "\n",
    "cost_function(x_train,y_train,w_init,b_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904045996674e-12\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters. \n",
    "cost = compute_cost(x_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.5578904045996674e-12)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost_function(x,y,w,b):\n",
    "    j=0\n",
    "    m=x.shape[0]\n",
    "    for i in range(m):\n",
    "        f_i=0\n",
    "        f_i=(np.dot(w,x[i])+b)\n",
    "        j=j+((f_i-y[i])**2)\n",
    "    j=j/(2*m)\n",
    "    return j\n",
    "\n",
    "cost_function(x_train,y_train,w_init,b_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: -1.6739251122999121e-06\n",
      "dj_dw at initial w,b: \n",
      " [-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05]\n"
     ]
    }
   ],
   "source": [
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(x_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m alpha = \u001b[32m5.0e-7\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# run gradient descent \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m w_final, b_final, J_hist = \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                                                    \u001b[49m\u001b[43mcompute_cost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_gradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                                                    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mb,w found by gradient descent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb_final\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw_final\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m m,_ = X_train.shape\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mgradient_descent\u001b[39m\u001b[34m(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# An array to store cost J and w's at each iteration primarily for graphing later\u001b[39;00m\n\u001b[32m     22\u001b[39m J_history = []\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m w = \u001b[43mcopy\u001b[49m.deepcopy(w_in)  \u001b[38;5;66;03m#avoid modifying global w within function\u001b[39;00m\n\u001b[32m     24\u001b[39m b = b_in\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Calculate the gradient and update the parameters\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(x_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(x_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'J_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# plot cost versus iteration  \u001b[39;00m\n\u001b[32m      2\u001b[39m fig, (ax1, ax2) = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, constrained_layout=\u001b[38;5;28;01mTrue\u001b[39;00m, figsize=(\u001b[32m12\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ax1.plot(\u001b[43mJ_hist\u001b[49m)\n\u001b[32m      4\u001b[39m ax2.plot(\u001b[32m100\u001b[39m + np.arange(\u001b[38;5;28mlen\u001b[39m(J_hist[\u001b[32m100\u001b[39m:])), J_hist[\u001b[32m100\u001b[39m:])\n\u001b[32m      5\u001b[39m ax1.set_title(\u001b[33m\"\u001b[39m\u001b[33mCost vs. iteration\u001b[39m\u001b[33m\"\u001b[39m);  ax2.set_title(\u001b[33m\"\u001b[39m\u001b[33mCost vs. iteration (tail)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'J_hist' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLsAAAGbCAYAAAAskpJqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIzNJREFUeJzt3X9s1fW9+PFXaWmr3tsuwqxFkJVd3djI3KUNjHLJMq/WoHEh2Y1dvBH1arJm20Xo1TsYNzqISbPdzNy5CW4TNEvQ2/gz/tHr6B/3YhXuD3rLsgwSF+Fa2FpJa2xRd4vA5/sHX/r9di3KOf3FeffxSM4f/fj59LzP3gNeeZ5fRVmWZQEAAAAACZg13QsAAAAAgIkidgEAAACQDLELAAAAgGSIXQAAAAAkQ+wCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJCMnGPXq6++GrfeemvMmzcvioqK4qWXXvrYa/bs2RO1tbVRXl4eixYtiscffzyftQIAJMuMBQAwMXKOXe+//35cd9118ZOf/OSCzj9y5EjcfPPNsWrVqujq6orvfve7sW7dunj++edzXiwAQKrMWAAAE6Moy7Is74uLiuLFF1+MNWvWnPec73znO/Hyyy/HoUOHho81NTXFr371q9i3b1++dw0AkCwzFgBA/kom+w727dsXDQ0NI47ddNNNsWPHjvjwww9j9uzZo64ZGhqKoaGh4Z/PnDkT77zzTsyZMyeKioome8kAACNkWRYnTpyIefPmxaxZF8dHnuYzY0WYswCAi8tkzFmTHrt6e3ujqqpqxLGqqqo4depU9PX1RXV19ahrWlpaYsuWLZO9NACAnBw9ejTmz58/3cuIiPxmrAhzFgBwcZrIOWvSY1dEjHqW8Nw7J8/37OGmTZuiubl5+OeBgYG4+uqr4+jRo1FRUTF5CwUAGMPg4GAsWLAg/vRP/3S6lzJCrjNWhDkLALi4TMacNemx68orr4ze3t4Rx44fPx4lJSUxZ86cMa8pKyuLsrKyUccrKioMYQDAtLmY3uaXz4wVYc4CAC5OEzlnTfqHTqxYsSLa29tHHNu9e3fU1dWd97MkAAD4aGYsAICx5Ry73nvvvThw4EAcOHAgIs5+7fWBAweiu7s7Is6+NH7t2rXD5zc1NcVbb70Vzc3NcejQodi5c2fs2LEj7r///ol5BAAACTBjAQBMjJzfxrh///74yle+Mvzzuc98uPPOO+Opp56Knp6e4aEsIqKmpiba2tpiw4YN8dhjj8W8efPi0Ucfja997WsTsHwAgDSYsQAAJkZRdu6TTC9ig4ODUVlZGQMDAz5LAgCYcinPIik/NgDg4jcZs8ikf2YXAAAAAEwVsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGSIXQAAAAAkQ+wCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGSIXQAAAAAkQ+wCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAycgrdm3bti1qamqivLw8amtro6Oj4yPP37VrV1x33XVx6aWXRnV1ddx9993R39+f14IBAFJmzgIAGJ+cY1dra2usX78+Nm/eHF1dXbFq1apYvXp1dHd3j3n+a6+9FmvXro177rknfvOb38Szzz4b//Vf/xX33nvvuBcPAJAScxYAwPjlHLseeeSRuOeee+Lee++NxYsXxz/90z/FggULYvv27WOe/+///u/xqU99KtatWxc1NTXxF3/xF/GNb3wj9u/fP+7FAwCkxJwFADB+OcWukydPRmdnZzQ0NIw43tDQEHv37h3zmvr6+jh27Fi0tbVFlmXx9ttvx3PPPRe33HLLee9naGgoBgcHR9wAAFJmzgIAmBg5xa6+vr44ffp0VFVVjTheVVUVvb29Y15TX18fu3btisbGxigtLY0rr7wyPvGJT8SPf/zj895PS0tLVFZWDt8WLFiQyzIBAAqOOQsAYGLk9QH1RUVFI37OsmzUsXMOHjwY69atiwcffDA6OzvjlVdeiSNHjkRTU9N5f/+mTZtiYGBg+Hb06NF8lgkAUHDMWQAA41OSy8lz586N4uLiUc8uHj9+fNSzkOe0tLTEypUr44EHHoiIiC984Qtx2WWXxapVq+Lhhx+O6urqUdeUlZVFWVlZLksDACho5iwAgImR0yu7SktLo7a2Ntrb20ccb29vj/r6+jGv+eCDD2LWrJF3U1xcHBFnn6kEAMCcBQAwUXJ+G2Nzc3M88cQTsXPnzjh06FBs2LAhuru7h18uv2nTpli7du3w+bfeemu88MILsX379jh8+HC8/vrrsW7duli2bFnMmzdv4h4JAECBM2cBAIxfTm9jjIhobGyM/v7+2Lp1a/T09MSSJUuira0tFi5cGBERPT090d3dPXz+XXfdFSdOnIif/OQn8Xd/93fxiU98Iq6//vr4/ve/P3GPAgAgAeYsAIDxK8oK4DXug4ODUVlZGQMDA1FRUTHdywEAZpiUZ5GUHxsAcPGbjFkkr29jBAAAAICLkdgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGSIXQAAAAAkQ+wCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGSIXQAAAAAkQ+wCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGSIXQAAAAAkI6/YtW3btqipqYny8vKora2Njo6Ojzx/aGgoNm/eHAsXLoyysrL49Kc/HTt37sxrwQAAKTNnAQCMT0muF7S2tsb69etj27ZtsXLlyvjpT38aq1evjoMHD8bVV1895jW33XZbvP3227Fjx474sz/7szh+/HicOnVq3IsHAEiJOQsAYPyKsizLcrlg+fLlsXTp0ti+ffvwscWLF8eaNWuipaVl1PmvvPJKfP3rX4/Dhw/H5ZdfntciBwcHo7KyMgYGBqKioiKv3wEAkK+pmkXMWQDATDMZs0hOb2M8efJkdHZ2RkNDw4jjDQ0NsXfv3jGvefnll6Ouri5+8IMfxFVXXRXXXntt3H///fGHP/zhvPczNDQUg4ODI24AACkzZwEATIyc3sbY19cXp0+fjqqqqhHHq6qqore3d8xrDh8+HK+99lqUl5fHiy++GH19ffHNb34z3nnnnfN+nkRLS0ts2bIll6UBABQ0cxYAwMTI6wPqi4qKRvycZdmoY+ecOXMmioqKYteuXbFs2bK4+eab45FHHomnnnrqvM86btq0KQYGBoZvR48ezWeZAAAFx5wFADA+Ob2ya+7cuVFcXDzq2cXjx4+PehbynOrq6rjqqquisrJy+NjixYsjy7I4duxYXHPNNaOuKSsri7KyslyWBgBQ0MxZAAATI6dXdpWWlkZtbW20t7ePON7e3h719fVjXrNy5cr4/e9/H++9997wsTfeeCNmzZoV8+fPz2PJAADpMWcBAEyMnN/G2NzcHE888UTs3LkzDh06FBs2bIju7u5oamqKiLMvjV+7du3w+bfffnvMmTMn7r777jh48GC8+uqr8cADD8Tf/M3fxCWXXDJxjwQAoMCZswAAxi+ntzFGRDQ2NkZ/f39s3bo1enp6YsmSJdHW1hYLFy6MiIienp7o7u4ePv9P/uRPor29Pf72b/826urqYs6cOXHbbbfFww8/PHGPAgAgAeYsAIDxK8qyLJvuRXycwcHBqKysjIGBgaioqJju5QAAM0zKs0jKjw0AuPhNxiyS17cxAgAAAMDFSOwCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGSIXQAAAAAkQ+wCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGSIXQAAAAAkQ+wCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLyil3btm2LmpqaKC8vj9ra2ujo6Lig615//fUoKSmJL37xi/ncLQBA8sxZAADjk3Psam1tjfXr18fmzZujq6srVq1aFatXr47u7u6PvG5gYCDWrl0bf/mXf5n3YgEAUmbOAgAYv6Isy7JcLli+fHksXbo0tm/fPnxs8eLFsWbNmmhpaTnvdV//+tfjmmuuieLi4njppZfiwIEDF3yfg4ODUVlZGQMDA1FRUZHLcgEAxm2qZhFzFgAw00zGLJLTK7tOnjwZnZ2d0dDQMOJ4Q0ND7N2797zXPfnkk/Hmm2/GQw89dEH3MzQ0FIODgyNuAAApM2cBAEyMnGJXX19fnD59OqqqqkYcr6qqit7e3jGv+e1vfxsbN26MXbt2RUlJyQXdT0tLS1RWVg7fFixYkMsyAQAKjjkLAGBi5PUB9UVFRSN+zrJs1LGIiNOnT8ftt98eW7ZsiWuvvfaCf/+mTZtiYGBg+Hb06NF8lgkAUHDMWQAA43NhTwH+X3Pnzo3i4uJRzy4eP3581LOQEREnTpyI/fv3R1dXV3z729+OiIgzZ85ElmVRUlISu3fvjuuvv37UdWVlZVFWVpbL0gAACpo5CwBgYuT0yq7S0tKora2N9vb2Ecfb29ujvr5+1PkVFRXx61//Og4cODB8a2pqis985jNx4MCBWL58+fhWDwCQCHMWAMDEyOmVXRERzc3Ncccdd0RdXV2sWLEifvazn0V3d3c0NTVFxNmXxv/ud7+LX/ziFzFr1qxYsmTJiOuvuOKKKC8vH3UcAGCmM2cBAIxfzrGrsbEx+vv7Y+vWrdHT0xNLliyJtra2WLhwYURE9PT0RHd394QvFAAgdeYsAIDxK8qyLJvuRXycwcHBqKysjIGBgaioqJju5QAAM0zKs0jKjw0AuPhNxiyS17cxAgAAAMDFSOwCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGSIXQAAAAAkQ+wCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGSIXQAAAAAkQ+wCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSkVfs2rZtW9TU1ER5eXnU1tZGR0fHec994YUX4sYbb4xPfvKTUVFREStWrIhf/vKXeS8YACBl5iwAgPHJOXa1trbG+vXrY/PmzdHV1RWrVq2K1atXR3d395jnv/rqq3HjjTdGW1tbdHZ2xle+8pW49dZbo6ura9yLBwBIiTkLAGD8irIsy3K5YPny5bF06dLYvn378LHFixfHmjVroqWl5YJ+x+c///lobGyMBx988ILOHxwcjMrKyhgYGIiKiopclgsAMG5TNYuYswCAmWYyZpGcXtl18uTJ6OzsjIaGhhHHGxoaYu/evRf0O86cORMnTpyIyy+//LznDA0NxeDg4IgbAEDKzFkAABMjp9jV19cXp0+fjqqqqhHHq6qqore394J+xw9/+MN4//3347bbbjvvOS0tLVFZWTl8W7BgQS7LBAAoOOYsAICJkdcH1BcVFY34OcuyUcfG8swzz8T3vve9aG1tjSuuuOK8523atCkGBgaGb0ePHs1nmQAABcecBQAwPiW5nDx37twoLi4e9ezi8ePHRz0L+cdaW1vjnnvuiWeffTZuuOGGjzy3rKwsysrKclkaAEBBM2cBAEyMnF7ZVVpaGrW1tdHe3j7ieHt7e9TX15/3umeeeSbuuuuuePrpp+OWW27Jb6UAAAkzZwEATIycXtkVEdHc3Bx33HFH1NXVxYoVK+JnP/tZdHd3R1NTU0ScfWn87373u/jFL34REWcHsLVr18aPfvSj+NKXvjT8bOUll1wSlZWVE/hQAAAKmzkLAGD8co5djY2N0d/fH1u3bo2enp5YsmRJtLW1xcKFCyMioqenJ7q7u4fP/+lPfxqnTp2Kb33rW/Gtb31r+Pidd94ZTz311PgfAQBAIsxZAADjV5RlWTbdi/g4g4ODUVlZGQMDA1FRUTHdywEAZpiUZ5GUHxsAcPGbjFkkr29jBAAAAICLkdgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGSIXQAAAAAkQ+wCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGSIXQAAAAAkQ+wCAAAAIBliFwAAAADJELsAAAAASIbYBQAAAEAyxC4AAAAAkiF2AQAAAJAMsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJEPsAgAAACAZYhcAAAAAyRC7AAAAAEiG2AUAAABAMsQuAAAAAJIhdgEAAACQDLELAAAAgGTkFbu2bdsWNTU1UV5eHrW1tdHR0fGR5+/Zsydqa2ujvLw8Fi1aFI8//nheiwUASJ05CwBgfHKOXa2trbF+/frYvHlzdHV1xapVq2L16tXR3d095vlHjhyJm2++OVatWhVdXV3x3e9+N9atWxfPP//8uBcPAJAScxYAwPgVZVmW5XLB8uXLY+nSpbF9+/bhY4sXL441a9ZES0vLqPO/853vxMsvvxyHDh0aPtbU1BS/+tWvYt++fRd0n4ODg1FZWRkDAwNRUVGRy3IBAMZtqmYRcxYAMNNMxixSksvJJ0+ejM7Ozti4ceOI4w0NDbF3794xr9m3b180NDSMOHbTTTfFjh074sMPP4zZs2ePumZoaCiGhoaGfx4YGIiIs/8DAABMtXMzSI7PEebEnAUAzESTMWflFLv6+vri9OnTUVVVNeJ4VVVV9Pb2jnlNb2/vmOefOnUq+vr6orq6etQ1LS0tsWXLllHHFyxYkMtyAQAmVH9/f1RWVk7K7zZnAQAz2UTOWTnFrnOKiopG/Jxl2ahjH3f+WMfP2bRpUzQ3Nw///O6778bChQuju7t70gZMJtbg4GAsWLAgjh496i0RBcS+FR57VnjsWWEaGBiIq6++Oi6//PJJvy9zFh/H3yOFx54VJvtWeOxZYZqMOSun2DV37twoLi4e9ezi8ePHRz2reM6VV1455vklJSUxZ86cMa8pKyuLsrKyUccrKyv9H7bAVFRU2LMCZN8Kjz0rPPasMM2aldcXWV8Qcxa58vdI4bFnhcm+FR57Vpgmcs7K6TeVlpZGbW1ttLe3jzje3t4e9fX1Y16zYsWKUefv3r076urqxvwcCQCAmcicBQAwMXLOZs3NzfHEE0/Ezp0749ChQ7Fhw4bo7u6OpqamiDj70vi1a9cOn9/U1BRvvfVWNDc3x6FDh2Lnzp2xY8eOuP/++yfuUQAAJMCcBQAwfjl/ZldjY2P09/fH1q1bo6enJ5YsWRJtbW2xcOHCiIjo6emJ7u7u4fNramqira0tNmzYEI899ljMmzcvHn300fja1752wfdZVlYWDz300JgvuefiZM8Kk30rPPas8NizwjRV+2bO4kLYs8JjzwqTfSs89qwwTca+FWWT+R3aAAAAADCFJu9TVgEAAABgioldAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIxkUTu7Zt2xY1NTVRXl4etbW10dHR8ZHn79mzJ2pra6O8vDwWLVoUjz/++BStlHNy2bMXXnghbrzxxvjkJz8ZFRUVsWLFivjlL385haslIvc/Z+e8/vrrUVJSEl/84hcnd4GMKdd9Gxoais2bN8fChQujrKwsPv3pT8fOnTunaLVE5L5nu3btiuuuuy4uvfTSqK6ujrvvvjv6+/unaLW8+uqrceutt8a8efOiqKgoXnrppY+9ppDmEDNWYTJnFR5zVmEyZxUec1ZhmbY5K7sI/PM//3M2e/bs7Oc//3l28ODB7L777ssuu+yy7K233hrz/MOHD2eXXnppdt9992UHDx7Mfv7zn2ezZ8/OnnvuuSle+cyV657dd9992fe///3sP//zP7M33ngj27RpUzZ79uzsv//7v6d45TNXrnt2zrvvvpstWrQoa2hoyK677rqpWSzD8tm3r371q9ny5cuz9vb27MiRI9l//Md/ZK+//voUrnpmy3XPOjo6slmzZmU/+tGPssOHD2cdHR3Z5z//+WzNmjVTvPKZq62tLdu8eXP2/PPPZxGRvfjiix95fiHNIWaswmTOKjzmrMJkzio85qzCM11z1kURu5YtW5Y1NTWNOPbZz34227hx45jn//3f/3322c9+dsSxb3zjG9mXvvSlSVsjI+W6Z2P53Oc+l23ZsmWil8Z55LtnjY2N2T/8wz9kDz30kCFsGuS6b//yL/+SVVZWZv39/VOxPMaQ65794z/+Y7Zo0aIRxx599NFs/vz5k7ZGzu9ChrBCmkPMWIXJnFV4zFmFyZxVeMxZhW0q56xpfxvjyZMno7OzMxoaGkYcb2hoiL179455zb59+0adf9NNN8X+/fvjww8/nLS1clY+e/bHzpw5EydOnIjLL798MpbIH8l3z5588sl4880346GHHprsJTKGfPbt5Zdfjrq6uvjBD34QV111VVx77bVx//33xx/+8IepWPKMl8+e1dfXx7Fjx6KtrS2yLIu33347nnvuubjlllumYsnkoVDmEDNWYTJnFR5zVmEyZxUec9bMMFGzSMlELyxXfX19cfr06aiqqhpxvKqqKnp7e8e8pre3d8zzT506FX19fVFdXT1p6yW/PftjP/zhD+P999+P2267bTKWyB/JZ89++9vfxsaNG6OjoyNKSqb9r4oZKZ99O3z4cLz22mtRXl4eL774YvT19cU3v/nNeOedd3yexBTIZ8/q6+tj165d0djYGP/7v/8bp06diq9+9avx4x//eCqWTB4KZQ4xYxUmc1bhMWcVJnNW4TFnzQwTNYtM+yu7zikqKhrxc5Zlo4593PljHWfy5Lpn5zzzzDPxve99L1pbW+OKK66YrOUxhgvds9OnT8ftt98eW7ZsiWuvvXaqlsd55PJn7cyZM1FUVBS7du2KZcuWxc033xyPPPJIPPXUU551nEK57NnBgwdj3bp18eCDD0ZnZ2e88sorceTIkWhqapqKpZKnQppDzFiFyZxVeMxZhcmcVXjMWembiFlk2p9GmDt3bhQXF48qscePHx9V88658sorxzy/pKQk5syZM2lr5ax89uyc1tbWuOeee+LZZ5+NG264YTKXyf8n1z07ceJE7N+/P7q6uuLb3/52RJz9xz3LsigpKYndu3fH9ddfPyVrn8ny+bNWXV0dV111VVRWVg4fW7x4cWRZFseOHYtrrrlmUtc80+WzZy0tLbFy5cp44IEHIiLiC1/4Qlx22WWxatWqePjhh72S5iJUKHOIGaswmbMKjzmrMJmzCo85a2aYqFlk2l/ZVVpaGrW1tdHe3j7ieHt7e9TX1495zYoVK0adv3v37qirq4vZs2dP2lo5K589izj7TONdd90VTz/9tPdIT7Fc96yioiJ+/etfx4EDB4ZvTU1N8ZnPfCYOHDgQy5cvn6qlz2j5/FlbuXJl/P73v4/33ntv+Ngbb7wRs2bNivnz50/qeslvzz744IOYNWvkP8fFxcUR8f+exeLiUihziBmrMJmzCo85qzCZswqPOWtmmLBZJKePs58k574+dMeOHdnBgwez9evXZ5dddln2P//zP1mWZdnGjRuzO+64Y/j8c19FuWHDhuzgwYPZjh07fC32FMt1z55++umspKQke+yxx7Kenp7h27vvvjtdD2HGyXXP/phvCZoeue7biRMnsvnz52d/9Vd/lf3mN7/J9uzZk11zzTXZvffeO10PYcbJdc+efPLJrKSkJNu2bVv25ptvZq+99lpWV1eXLVu2bLoewoxz4sSJrKurK+vq6soiInvkkUeyrq6u4a8xL+Q5xIxVmMxZhcecVZjMWYXHnFV4pmvOuihiV5Zl2WOPPZYtXLgwKy0tzZYuXZrt2bNn+L/deeed2Ze//OUR5//bv/1b9ud//udZaWlp9qlPfSrbvn37FK+YXPbsy1/+chYRo2533nnn1C98Bsv1z9n/zxA2fXLdt0OHDmU33HBDdskll2Tz58/Pmpubsw8++GCKVz2z5bpnjz76aPa5z30uu+SSS7Lq6ursr//6r7Njx45N8apnrn/913/9yH+jCn0OMWMVJnNW4TFnFSZzVuExZxWW6ZqzirLMa/cAAAAASMO0f2YXAAAAAEwUsQsAAACAZIhdAAAAACRD7AIAAAAgGWIXAAAAAMkQuwAAAABIhtgFAAAAQDLELgAAAACSIXYBAAAAkAyxCwAAAIBkiF0AAAAAJOP/AJqE0OP66ZU2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist)\n",
    "ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandiayyapamuruga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
